{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessari:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flickrapi import FlickrAPI\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "import urllib\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torchnet.meter import AverageValueMeter\n",
    "from torchnet.logger import VisdomPlotLogger, VisdomSaver\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dichiarazione costanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "#Media e deviazione standard precalcolata per Squeezenet.\n",
    "mean_squeezenet = [0.485, 0.456, 0.406]\n",
    "std_squeezenet = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dichiarazione di classi e funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenesDataset(Dataset):\n",
    "    def __init__(self,base_path,txt_list,transform=None):\n",
    "        #conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path=base_path\n",
    "        #carichiamo la lista dei file\n",
    "        #sarÃ  una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        #conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        #recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "        #carichiamo l'immagine utilizzando PIL\n",
    "        im = Image.open(path.join(self.base_path, f)).convert('RGB')\n",
    "        #Resize:\n",
    "        im = im.resize((width,height))\n",
    "        #se la trasfromazione Ã¨ definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        #convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "        #restituiamo un dizionario contenente immagine etichetta\n",
    "        return {'image' : im, 'label':label}\n",
    "    #restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dato un oggetto di tipo ScenesDataset restituisce la media e la deviazione standard calcolata su tutte le immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_devst(dataset):\n",
    "    m = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        m+= np.array(sample['image'].sum(1).sum(1)) #accumuliamo la somma dei pixel canale per canale\n",
    "    #dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "    m=m/(len(dataset)*width*height)\n",
    "    #procedura simile per calcolare la deviazione standard\n",
    "    s = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        s+= np.array(((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1))\n",
    "    s=np.sqrt(s/(len(dataset)*width*height))\n",
    "    #print(\"Medie\",m)\n",
    "    #print(\"Dev.Std.\",s)\n",
    "    return m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsImg(name):\n",
    "    if str.__contains__(name, \".jpg\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download immagini da flickr tramite Api search.\n",
    "- Inserire FLICKR_PUBLIC e FLICKR_SECRET personali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat number--> 6000 saved.\n",
      "Cat number--> 6001 saved.\n",
      "Cat number--> 6002 saved.\n",
      "Cat number--> 6003 saved.\n",
      "Cat number--> 6004 saved.\n",
      "Cat number--> 6005 saved.\n",
      "Cat number--> 6006 saved.\n",
      "Cat number--> 6007 saved.\n",
      "Cat number--> 6008 saved.\n",
      "Cat number--> 6009 saved.\n",
      "Cat number--> 6010 saved.\n",
      "Cat number--> 6011 saved.\n",
      "Cat number--> 6012 saved.\n",
      "Cat number--> 6013 saved.\n",
      "Cat number--> 6014 saved.\n",
      "Cat number--> 6015 saved.\n",
      "Cat number--> 6016 saved.\n",
      "Cat number--> 6017 saved.\n",
      "Cat number--> 6018 saved.\n",
      "Cat number--> 6019 saved.\n",
      "Cat number--> 6020 saved.\n",
      "Cat number--> 6021 saved.\n",
      "Cat number--> 6022 saved.\n",
      "Cat number--> 6023 saved.\n",
      "Cat number--> 6024 saved.\n",
      "Cat number--> 6025 saved.\n",
      "Cat number--> 6026 saved.\n",
      "Cat number--> 6027 saved.\n",
      "Cat number--> 6028 saved.\n",
      "Cat number--> 6029 saved.\n",
      "Cat number--> 6030 saved.\n",
      "Cat number--> 6031 saved.\n",
      "Cat number--> 6032 saved.\n",
      "Cat number--> 6033 saved.\n",
      "Cat number--> 6034 saved.\n",
      "Cat number--> 6035 saved.\n",
      "Cat number--> 6036 saved.\n",
      "Cat number--> 6037 saved.\n",
      "Cat number--> 6038 saved.\n",
      "Cat number--> 6039 saved.\n",
      "Cat number--> 6040 saved.\n",
      "Cat number--> 6041 saved.\n",
      "Cat number--> 6042 saved.\n",
      "Cat number--> 6043 saved.\n",
      "Cat number--> 6044 saved.\n",
      "Cat number--> 6045 saved.\n",
      "Cat number--> 6046 saved.\n",
      "Cat number--> 6047 saved.\n",
      "Cat number--> 6048 saved.\n",
      "Cat number--> 6049 saved.\n",
      "Cat number--> 6050 saved.\n",
      "Cat number--> 6051 saved.\n",
      "Cat number--> 6052 saved.\n",
      "Cat number--> 6053 saved.\n",
      "Cat number--> 6054 saved.\n",
      "Cat number--> 6055 saved.\n",
      "Cat number--> 6056 saved.\n",
      "Cat number--> 6057 saved.\n",
      "Cat number--> 6058 saved.\n",
      "Cat number--> 6059 saved.\n",
      "Cat number--> 6060 saved.\n",
      "Cat number--> 6061 saved.\n",
      "Cat number--> 6062 saved.\n",
      "Cat number--> 6063 saved.\n",
      "Cat number--> 6064 saved.\n",
      "Cat number--> 6065 saved.\n",
      "Cat number--> 6066 saved.\n",
      "Cat number--> 6067 saved.\n",
      "Cat number--> 6068 saved.\n",
      "Cat number--> 6069 saved.\n",
      "Cat number--> 6070 saved.\n",
      "Cat number--> 6071 saved.\n",
      "Cat number--> 6072 saved.\n",
      "Cat number--> 6073 saved.\n",
      "Cat number--> 6074 saved.\n",
      "Cat number--> 6075 saved.\n",
      "Cat number--> 6076 saved.\n",
      "Cat number--> 6077 saved.\n",
      "Cat number--> 6078 saved.\n",
      "Cat number--> 6079 saved.\n",
      "Cat number--> 6080 saved.\n",
      "Cat number--> 6081 saved.\n",
      "Cat number--> 6082 saved.\n",
      "Cat number--> 6083 saved.\n",
      "Cat number--> 6084 saved.\n",
      "Cat number--> 6085 saved.\n",
      "Cat number--> 6086 saved.\n",
      "Cat number--> 6087 saved.\n",
      "Cat number--> 6088 saved.\n",
      "Cat number--> 6089 saved.\n",
      "Cat number--> 6090 saved.\n",
      "Cat number--> 6091 saved.\n",
      "Cat number--> 6092 saved.\n",
      "Cat number--> 6093 saved.\n",
      "Cat number--> 6094 saved.\n",
      "Cat number--> 6095 saved.\n",
      "Cat number--> 6096 saved.\n",
      "Cat number--> 6097 saved.\n",
      "Cat number--> 6098 saved.\n",
      "Cat number--> 6099 saved.\n",
      "Cat number--> 6100 saved.\n",
      "Cat number--> 6101 saved.\n",
      "Cat number--> 6102 saved.\n",
      "Cat number--> 6103 saved.\n",
      "Cat number--> 6104 saved.\n",
      "Cat number--> 6105 saved.\n",
      "Cat number--> 6106 saved.\n",
      "Cat number--> 6107 saved.\n",
      "Cat number--> 6108 saved.\n",
      "Cat number--> 6109 saved.\n",
      "Cat number--> 6110 saved.\n",
      "Cat number--> 6111 saved.\n",
      "Cat number--> 6112 saved.\n",
      "Cat number--> 6113 saved.\n",
      "Cat number--> 6114 saved.\n",
      "Cat number--> 6115 saved.\n",
      "Cat number--> 6116 saved.\n",
      "Cat number--> 6117 saved.\n",
      "Cat number--> 6118 saved.\n",
      "Cat number--> 6119 saved.\n",
      "Cat number--> 6120 saved.\n",
      "Cat number--> 6121 saved.\n",
      "Cat number--> 6122 saved.\n",
      "Cat number--> 6123 saved.\n",
      "Cat number--> 6124 saved.\n",
      "Cat number--> 6125 saved.\n",
      "Cat number--> 6126 saved.\n",
      "Cat number--> 6127 saved.\n",
      "Cat number--> 6128 saved.\n",
      "Cat number--> 6129 saved.\n",
      "Cat number--> 6130 saved.\n",
      "Cat number--> 6131 saved.\n",
      "Cat number--> 6132 saved.\n",
      "Cat number--> 6133 saved.\n",
      "Cat number--> 6134 saved.\n",
      "Cat number--> 6135 saved.\n",
      "Cat number--> 6136 saved.\n",
      "Cat number--> 6137 saved.\n",
      "Cat number--> 6138 saved.\n",
      "Cat number--> 6139 saved.\n",
      "Cat number--> 6140 saved.\n",
      "Cat number--> 6141 saved.\n",
      "Cat number--> 6142 saved.\n",
      "Cat number--> 6143 saved.\n",
      "Cat number--> 6144 saved.\n",
      "Cat number--> 6145 saved.\n",
      "Cat number--> 6146 saved.\n",
      "Cat number--> 6147 saved.\n",
      "Cat number--> 6148 saved.\n",
      "Cat number--> 6149 saved.\n",
      "Cat number--> 6150 saved.\n",
      "Cat number--> 6151 saved.\n",
      "Cat number--> 6152 saved.\n",
      "Cat number--> 6153 saved.\n",
      "Cat number--> 6154 saved.\n",
      "Cat number--> 6155 saved.\n",
      "Cat number--> 6156 saved.\n",
      "Cat number--> 6157 saved.\n",
      "Cat number--> 6158 saved.\n",
      "Cat number--> 6159 saved.\n",
      "Cat number--> 6160 saved.\n",
      "Cat number--> 6161 saved.\n",
      "Cat number--> 6162 saved.\n",
      "Cat number--> 6163 saved.\n",
      "Cat number--> 6164 saved.\n",
      "Cat number--> 6165 saved.\n",
      "Cat number--> 6166 saved.\n",
      "Cat number--> 6167 saved.\n",
      "Cat number--> 6168 saved.\n",
      "Cat number--> 6169 saved.\n",
      "Cat number--> 6170 saved.\n",
      "Cat number--> 6171 saved.\n",
      "Cat number--> 6172 saved.\n",
      "Cat number--> 6173 saved.\n",
      "Cat number--> 6174 saved.\n",
      "Cat number--> 6175 saved.\n",
      "Cat number--> 6176 saved.\n",
      "Cat number--> 6177 saved.\n",
      "Cat number--> 6178 saved.\n",
      "Cat number--> 6179 saved.\n",
      "Cat number--> 6180 saved.\n",
      "Cat number--> 6181 saved.\n",
      "Cat number--> 6182 saved.\n",
      "Cat number--> 6183 saved.\n",
      "Cat number--> 6184 saved.\n",
      "Cat number--> 6185 saved.\n",
      "Cat number--> 6186 saved.\n",
      "Cat number--> 6187 saved.\n",
      "Cat number--> 6188 saved.\n",
      "Cat number--> 6189 saved.\n",
      "Cat number--> 6190 saved.\n",
      "Cat number--> 6191 saved.\n",
      "Cat number--> 6192 saved.\n",
      "Cat number--> 6193 saved.\n",
      "Cat number--> 6194 saved.\n",
      "Cat number--> 6195 saved.\n",
      "Cat number--> 6196 saved.\n",
      "Cat number--> 6197 saved.\n",
      "Cat number--> 6198 saved.\n",
      "Cat number--> 6199 saved.\n",
      "Cat number--> 6200 saved.\n",
      "Cat number--> 6201 saved.\n",
      "Cat number--> 6202 saved.\n",
      "Cat number--> 6203 saved.\n",
      "Cat number--> 6204 saved.\n",
      "Cat number--> 6205 saved.\n",
      "Cat number--> 6206 saved.\n",
      "Cat number--> 6207 saved.\n",
      "Cat number--> 6208 saved.\n",
      "Cat number--> 6209 saved.\n",
      "Cat number--> 6210 saved.\n",
      "Cat number--> 6211 saved.\n",
      "Cat number--> 6212 saved.\n",
      "Cat number--> 6213 saved.\n",
      "Cat number--> 6214 saved.\n",
      "Cat number--> 6215 saved.\n",
      "Cat number--> 6216 saved.\n",
      "Cat number--> 6217 saved.\n",
      "Cat number--> 6218 saved.\n",
      "Cat number--> 6219 saved.\n",
      "Cat number--> 6220 saved.\n",
      "Cat number--> 6221 saved.\n",
      "Cat number--> 6222 saved.\n",
      "Cat number--> 6223 saved.\n",
      "Cat number--> 6224 saved.\n",
      "Cat number--> 6225 saved.\n",
      "Cat number--> 6226 saved.\n",
      "Cat number--> 6227 saved.\n",
      "Cat number--> 6228 saved.\n",
      "Cat number--> 6229 saved.\n",
      "Cat number--> 6230 saved.\n",
      "Cat number--> 6231 saved.\n",
      "Cat number--> 6232 saved.\n",
      "Cat number--> 6233 saved.\n",
      "Cat number--> 6234 saved.\n",
      "Cat number--> 6235 saved.\n",
      "Cat number--> 6236 saved.\n",
      "Cat number--> 6237 saved.\n",
      "Cat number--> 6238 saved.\n",
      "Cat number--> 6239 saved.\n",
      "Cat number--> 6240 saved.\n",
      "Cat number--> 6241 saved.\n",
      "Cat number--> 6242 saved.\n",
      "Cat number--> 6243 saved.\n",
      "Cat number--> 6244 saved.\n",
      "Cat number--> 6245 saved.\n",
      "Cat number--> 6246 saved.\n",
      "Cat number--> 6247 saved.\n",
      "Cat number--> 6248 saved.\n",
      "Cat number--> 6249 saved.\n",
      "Cat number--> 6250 saved.\n",
      "Cat number--> 6251 saved.\n",
      "Cat number--> 6252 saved.\n",
      "Cat number--> 6253 saved.\n",
      "Cat number--> 6254 saved.\n",
      "Cat number--> 6255 saved.\n",
      "Cat number--> 6256 saved.\n",
      "Cat number--> 6257 saved.\n",
      "Cat number--> 6258 saved.\n",
      "Cat number--> 6259 saved.\n",
      "Cat number--> 6260 saved.\n",
      "Cat number--> 6261 saved.\n",
      "Cat number--> 6262 saved.\n",
      "Cat number--> 6263 saved.\n",
      "Cat number--> 6264 saved.\n",
      "Cat number--> 6265 saved.\n",
      "Cat number--> 6266 saved.\n",
      "Cat number--> 6267 saved.\n",
      "Cat number--> 6268 saved.\n",
      "Cat number--> 6269 saved.\n",
      "Cat number--> 6270 saved.\n",
      "Cat number--> 6271 saved.\n",
      "Cat number--> 6272 saved.\n",
      "Cat number--> 6273 saved.\n",
      "Cat number--> 6274 saved.\n",
      "Cat number--> 6275 saved.\n",
      "Cat number--> 6276 saved.\n",
      "Cat number--> 6277 saved.\n",
      "Cat number--> 6278 saved.\n",
      "Cat number--> 6279 saved.\n",
      "Cat number--> 6280 saved.\n",
      "Cat number--> 6281 saved.\n",
      "Cat number--> 6282 saved.\n",
      "Cat number--> 6283 saved.\n",
      "Cat number--> 6284 saved.\n",
      "Cat number--> 6285 saved.\n",
      "Cat number--> 6286 saved.\n",
      "Cat number--> 6287 saved.\n",
      "Cat number--> 6288 saved.\n",
      "Cat number--> 6289 saved.\n",
      "Cat number--> 6290 saved.\n",
      "Cat number--> 6291 saved.\n",
      "Cat number--> 6292 saved.\n",
      "Cat number--> 6293 saved.\n",
      "Cat number--> 6294 saved.\n",
      "Cat number--> 6295 saved.\n",
      "Cat number--> 6296 saved.\n"
     ]
    }
   ],
   "source": [
    "FLICKR_PUBLIC = 'fdb1863f5162502cf08eafc41ddc43e5'\n",
    "FLICKR_SECRET = '1bcbf94ddbf054d9'\n",
    "\n",
    "flickr = FlickrAPI(FLICKR_PUBLIC, FLICKR_SECRET, format='parsed-json')\n",
    "extras='url_m'\n",
    "\n",
    "for pg in range(13,43):\n",
    "    cats = flickr.photos.search(text='cat', page=pg, per_page=500, extras=extras, safe_search=1, content_type = 1, sort=\"interestingness-desc\")\n",
    "    photos_cats = cats['photos']\n",
    "    if pg == 1:\n",
    "        count = 0\n",
    "    else:\n",
    "        count = pg*500-501\n",
    "    for i in cats['photos']['photo']:\n",
    "        try:\n",
    "            count += 1\n",
    "            url = i['url_m']\n",
    "            image = io.imread(url)\n",
    "            #io.imshow(image)\n",
    "            #io.show()\n",
    "            io.imsave(\"ImmaginiMaiViste/cat/cat_\" + str(count) + \".jpg\", np.array(image))\n",
    "            print(\"Cat number-->\", count, \"saved.\")\n",
    "        except:\n",
    "            print(\"*******************\")\n",
    "            print(\"Errore...\")\n",
    "            print(\"*******************\")\n",
    "    \n",
    "    dogs = flickr.photos.search(text='dog', page=pg, per_page=500, extras=extras, safe_search=1, content_type = 1, sort=\"interestingness-desc\")\n",
    "    photos_dogs = dogs['photos']\n",
    "    if pg == 1:\n",
    "        count = 0\n",
    "    else:\n",
    "        count = pg*500-501\n",
    "    for i in dogs['photos']['photo']:\n",
    "        try:\n",
    "            count += 1\n",
    "            url = i['url_m']\n",
    "            image = io.imread(url)\n",
    "            #io.imshow(image)\n",
    "            #io.show()\n",
    "            io.imsave(\"ImmaginiMaiViste/dog/dog_\" + str(count) + \".jpg\", np.array(image))\n",
    "            print(\"Dog number-->\", count, \"saved.\")\n",
    "        except:\n",
    "            print(\"*******************\")\n",
    "            print(\"Errore...\")\n",
    "            print(\"*******************\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo i file train e test txt con le relative classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"train_immagini_mai_viste.txt\",\"w+\")\n",
    "test = open(\"test_immagini_mai_viste.txt\",\"w+\")\n",
    "random.seed(1234)\n",
    "entries = os.listdir('ImmaginiMaiViste/cat/')\n",
    "stop = 0\n",
    "for entry in entries:\n",
    "    #if( stop == 200):\n",
    "        #break\n",
    "    stop += 1\n",
    "    if IsImg(entry):\n",
    "        #if(random.randint(1, 100) <= 70):\n",
    "        train.write(\"ImmaginiMaiViste/cat/\" + entry + \", \" + str(0) +\"\\n\")\n",
    "        #else:\n",
    "            #test.write(\"ImmaginiMaiViste/cat/\" + entry + \", \" + str(0) + \"\\n\")\n",
    "\n",
    "entries = os.listdir('ImmaginiMaiViste/dog/')\n",
    "stop = 0\n",
    "for entry in entries:\n",
    "    #if( stop == 200):\n",
    "        #break\n",
    "    stop += 1\n",
    "    if IsImg(entry):        \n",
    "        #if(random.randint(1, 100) <= 70):\n",
    "        train.write(\"ImmaginiMaiViste/dog/\" + entry + \", \" + str(1) +\"\\n\")\n",
    "        #else:\n",
    "        #test.write(\"ImmaginiMaiViste/dog/\" + entry + \", \" + str(1) +\"\\n\")\n",
    "\n",
    "train.close()\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo l'oggetto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decommentare per ricalcolare se cambiano le immagini.\n",
    "dataset_train = ScenesDataset('','train.txt',transform=transforms.ToTensor())\n",
    "#mean_pre_trained, std_pre_trained = get_mean_devst(dataset_train)\n",
    "\n",
    "#Database normalizzato per squeezenet preallenata\n",
    "#print(mean_pre_trained)\n",
    "#print(std_pre_trained)\n",
    "mean_pre_trained = [0.47607468, 0.44641235, 0.39604222]\n",
    "std_pre_trained = [0.27902635, 0.27250564, 0.27781429]\n",
    "\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_squeezenet, std_squeezenet),\n",
    "                                  ])\n",
    "dataset_train_squeezenet = ScenesDataset('','train.txt',transform=transformss)\n",
    "dataset_test_squeezenet = ScenesDataset('','test.txt',transform=transformss)\n",
    "\n",
    "#Database normalizzato per regressione logistica.\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_pre_trained, std_pre_trained),\n",
    "                                  ])\n",
    "dataset_train_logistic_regression = ScenesDataset('','train.txt',transform=transformss)\n",
    "dataset_test_logistic_regression = ScenesDataset('','test.txt',transform=transformss)\n",
    "\n",
    "\n",
    "'''\n",
    "sample = dataset_train[0]\n",
    "#l'immagine Ã¨ 3 x 256 x 256 perchÃ© Ã¨ una immagine a colori\n",
    "print(sample)\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo dataset per calcolo accuracy su immagini mai viste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pre_trained = [0.47607468, 0.44641235, 0.39604222]\n",
    "std_pre_trained = [0.27902635, 0.27250564, 0.27781429]\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_squeezenet, std_squeezenet),\n",
    "                                  ])\n",
    "dataset_mai_viste_squeezenet = ScenesDataset('','train_immagini_mai_viste.txt',transform=transformss)\n",
    "\n",
    "#Database normalizzato per regressione logistica.\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_pre_trained, std_pre_trained),\n",
    "                                  ])\n",
    "dataset_mai_viste_logistic_regression = ScenesDataset('','train_immagini_mai_viste.txt',transform=transformss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo l'oggetto DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_squeezenet = DataLoader(dataset_train_squeezenet, batch_size=20, num_workers=0, shuffle=True)\n",
    "test_loader_squeezenet = DataLoader(dataset_test_squeezenet, batch_size=20, num_workers=0)\n",
    "train_loader_mai_viste_squeezenet = DataLoader(dataset_mai_viste_squeezenet, batch_size=20, num_workers=0)\n",
    "\n",
    "train_loader_logistic_regression = DataLoader(dataset_train_logistic_regression, batch_size=20, num_workers=0, shuffle=True)\n",
    "test_loader_logistic_regression = DataLoader(dataset_test_logistic_regression, batch_size=20, num_workers=0)\n",
    "train_loader_mai_viste_logistic_regression = DataLoader(dataset_mai_viste_logistic_regression, batch_size=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, in_features, ou_features):\n",
    "        \"\"\"Costruisce un regressore logistico.\n",
    "        Input:\n",
    "        in_features: numero di feature in input (es. 30)\"\"\"\n",
    "        super(LogisticRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo Ã¨ necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_features, ou_features) #il regressore logistico restituisce probabilitÃ \n",
    "        #quindi il numero di feature di output Ã¨ \"1\"\n",
    "        self.logistic = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        logits = self.linear(x)\n",
    "        if self.training: #se il modulo Ã¨ in fase di training\n",
    "        #la proprietÃ  \"training\" Ã¨ messa a disposizione dai meccanismi\n",
    "        #automatici dei moduli di PyTorch\n",
    "            return logits\n",
    "        else: #se siamo in fase di test, calcoliamo le probabilitÃ \n",
    "            return self.logistic(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variabili di controllo per l'allenamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(dataset_train) / batch_size)\n",
    "input_dim = width*height*3\n",
    "output_dim = 2\n",
    "lr_rate = 0.0001\n",
    "channel = 3\n",
    "momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_regression = LogisticRegression(input_dim, output_dim)\n",
    "model_squeezenet = models.squeezenet1_0(pretrained=True)#(input_dim, output_dim)\n",
    "model_alexnet = models.alexnet(pretrained=True)#(input_dim, output_dim)\n",
    "model_alexnet.classifier[6] = nn.Linear(4096, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(model, train_loader, test_loader, lr=0.001, epochs=20, momentum=0.8, exp_name = 'experiment', logistic_regression = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(),lr, momentum=momentum)\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "    \n",
    "    \n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    loss_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Loss', 'legend':['train','test']})\n",
    "    acc_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Accuracy','legend':['train','test']})\n",
    "    visdom_saver = VisdomSaver(envs=[exp_name])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        #print(\"Primo ciclo for.\")\n",
    "        for mode in ['train', 'test']:\n",
    "            #print(\"Secondo ciclo for.\")\n",
    "            \n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "            \n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "            #print(\"Mode-->\",mode)\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                #trasformiamo i tensori in variabili\n",
    "                if logistic_regression == True:\n",
    "                    x=Variable(batch['image'].view(-1, width*height*channel), requires_grad=(mode=='train'))\n",
    "                else:\n",
    "                    x=Variable(batch['image'], requires_grad=(mode=='train'))\n",
    "\n",
    "                y=Variable(batch['label'])\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                    #print(\"Con cuda\")\n",
    "                #else:\n",
    "                    #print(\"Senza cuda\")\n",
    "                output = model(x)\n",
    "                #print(output)\n",
    "                #print(y)\n",
    "                l = criterion(output,y)\n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                #print(\"L-->\",l.item())\n",
    "                acc = accuracy_score(y.cpu().data,output.cpu().max(1)[1].data)\n",
    "                epoch_loss+=l.data.item()*x.shape[0]\n",
    "                epoch_acc+=acc*x.shape[0]\n",
    "                samples+=x.shape[0]\n",
    "                '''print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "                (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples),\n",
    "                epoch_loss/samples,\n",
    "                epoch_acc/samples,\n",
    "                losses[mode].append(epoch_loss))'''\n",
    "                accuracies[mode].append(epoch_acc)\n",
    "                n = batch['image'].shape[0]\n",
    "                loss_meter.add(l.item()*n,n)\n",
    "                acc_meter.add(acc*n,n)\n",
    "                #loss_logger.log(e+(i+1)/len(loaders[mode]), loss_meter.value()[0], name=mode)\n",
    "                #acc_logger.log(e+(i+1)/len(loaders[mode]), acc_meter.value()[0], name=mode)\n",
    "\n",
    "\n",
    "            loss_logger.log(e+1, loss_meter.value()[0], name=mode)\n",
    "            acc_logger.log(e+1, acc_meter.value()[0], name=mode)\n",
    "            if mode == \"train\":\n",
    "                print(e, end = \" \")\n",
    "            #print(\"Fine secondo ciclo for\")\n",
    "        '''print(\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "        (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))'''\n",
    "        torch.save(model.state_dict(), \"./\" + exp_name + \".pth\")\n",
    "        print(\"Pesi aggiornati.\")\n",
    "    #print(\"Ho finito.\")\n",
    "    #restituiamo il modello e i vari log\n",
    "    return model, (losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(model_logistic_regression, \n",
    "                                                     epochs = 50, \n",
    "                                                     train_loader = train_loader_logistic_regression,\n",
    "                                                     test_loader = test_loader_logistic_regression, \n",
    "                                                     logistic_regression = True,\n",
    "                                                     exp_name = \"Train_logisticRegression_lr\" + str(lr_rate) + \"_momentum\" + str(momentum) + \"_epochs\" + str(int(epochs)), \n",
    "                                                     lr = lr_rate, \n",
    "                                                     momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_squeezenet, result_logs_squeezenet = train_classification(model_squeezenet, \n",
    "                                                     epochs = 50, \n",
    "                                                     train_loader = train_loader_squeezenet,\n",
    "                                                     test_loader = test_loader_squeezenet, \n",
    "                                                     logistic_regression = False,\n",
    "                                                     exp_name = \"Train_squeezenet_lr\" + str(lr_rate) + \"_momentum\" + str(momentum) + \"_epochs\" + str(int(epochs)), \n",
    "                                                     lr = lr_rate, \n",
    "                                                     momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_alexnet, result_logs_alexnet = train_classification(model_alexnet, \n",
    "                                                     epochs = 50, \n",
    "                                                     train_loader = train_loader_squeezenet,\n",
    "                                                     test_loader = test_loader_squeezenet, \n",
    "                                                     logistic_regression = False,\n",
    "                                                     exp_name = \"NuovaProva_Train_alexnet_lr\" + str(lr_rate) + \"_momentum\" + str(momentum) + \"_epochs\" + str(int(epochs)), \n",
    "                                                     lr = lr_rate, \n",
    "                                                     momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(net, path):\n",
    "    my_file = Path(path) #Carica il file(lo state potenzialmente)\n",
    "    if my_file.is_file(): #Carico lo state altrimenti rialleno\n",
    "        net.load_state_dict(torch.load(path))\n",
    "        print(\"Caricato \" + path)\n",
    "    else: \n",
    "        print(\"File mancante.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state(model_alexnet, \"./Train_alexnet_lr0.0001_momentum0.8_epochs53.pth\")\n",
    "load_state(model_squeezenet, \"./Train_squeezenet_lr0.0001_momentum0.8_epochs53.pth\")\n",
    "load_state(model_logistic_regression, \"./Train_logisticRegression_lr0.0001_momentum0.8_epochs53.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_classification(model, test_loader, logistic_regression = True):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch in test_loader:\n",
    "        #x=Variable(batch[\"image\"])\n",
    "        if logistic_regression == True:\n",
    "            x=Variable(batch['image'].view(-1, width*height*channel))\n",
    "        else:\n",
    "            x=Variable(batch['image'])\n",
    "        x = x.cpu()\n",
    "        #applichiamo la funzione softmax per avere delle probabilitÃ \n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            model.cuda()\n",
    "        pred = softmax(model(x)).data.cpu().numpy().copy()\n",
    "        gt = batch[\"label\"].cpu().numpy().copy()\n",
    "        #print(\"Pred-->\", pred, \", gt-->\", gt)\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "        #print(len(preds), len(gts))\n",
    "    return np.concatenate(preds),np.concatenate(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators(net, loader, train, logistic_regression):\n",
    "    #In questo caso e' l'accuracy per la classe AlexNetRetrain\n",
    "    predictions, groundruth_t = test_model_classification(net, loader, logistic_regression)\n",
    "    if train == True:\n",
    "        print(\"****Indicatori per dataset di Train****\")\n",
    "    else:\n",
    "        print(\"****Indicatori per dataset di Test****\")\n",
    "    print (\"Accuracy: %0.2f\" % \\\n",
    "    accuracy_score(groundruth_t,predictions.argmax(1)))\n",
    "    print(\"Precision_score-->\")\n",
    "    print(precision_score(groundruth_t,predictions.argmax(1), average = 'macro'))\n",
    "    print(\"Recall_score-->\")\n",
    "    print(recall_score(groundruth_t,predictions.argmax(1), average = 'macro'))\n",
    "    print(\"F1_Score-->\")\n",
    "    print(f1_score(groundruth_t,predictions.argmax(1), average = 'macro'))\n",
    "    print(\"Matrice di confusione-->\\n\")\n",
    "    print(confusion_matrix(groundruth_t,predictions.argmax(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------ Accuracy su immagini mai viste prima ------------------------------------------\")\n",
    "print(\"++++++++++ Logistic Regression ++++++++++\")\n",
    "get_indicators(model_logistic_regression, train_loader_mai_viste_logistic_regression, True, True)\n",
    "print(\"++++++++++ Squeezenet ++++++++++\")\n",
    "get_indicators(model_squeezenet, train_loader_mai_viste_squeezenet, True, False)\n",
    "print(\"++++++++++ AlexNet ++++++++++\")\n",
    "get_indicators(model_alexnet, train_loader_mai_viste_squeezenet, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------ Accuracy su immagini di train ------------------------------------------\")\n",
    "print(\"++++++++++ Logistic Regression ++++++++++\")\n",
    "get_indicators(model_logistic_regression, train_loader_logistic_regression, True, True)\n",
    "print(\"++++++++++ Squeezenet ++++++++++\")\n",
    "get_indicators(model_squeezenet, train_loader_squeezenet, True, False)\n",
    "print(\"++++++++++ AlexNet ++++++++++\")\n",
    "get_indicators(model_alexnet, train_loader_squeezenet, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------ Accuracy su immagini di test ------------------------------------------\")\n",
    "print(\"++++++++++ Logistic Regression ++++++++++\")\n",
    "get_indicators(model_logistic_regression, test_loader_logistic_regression, False, True)\n",
    "print(\"++++++++++ Squeezenet ++++++++++\")\n",
    "get_indicators(model_squeezenet, test_loader_squeezenet, False, False)\n",
    "print(\"++++++++++ AlexNet ++++++++++\")\n",
    "get_indicators(model_alexnet, test_loader_squeezenet, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
