{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessari:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flickrapi import FlickrAPI\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "import urllib\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torchnet.meter import AverageValueMeter\n",
    "from torchnet.logger import VisdomPlotLogger, VisdomSaver\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dichiarazione costanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "#Media e deviazione standard precalcolata per Squeezenet.\n",
    "mean_squeezenet = [0.485, 0.456, 0.406]\n",
    "std_squeezenet = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dichiarazione di classi e funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenesDataset(Dataset):\n",
    "    def __init__(self,base_path,txt_list,transform=None):\n",
    "        #conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path=base_path\n",
    "        #carichiamo la lista dei file\n",
    "        #sarà una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        #conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        #recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "        #carichiamo l'immagine utilizzando PIL\n",
    "        im = Image.open(path.join(self.base_path, f)).convert('RGB')\n",
    "        #Resize:\n",
    "        im = im.resize((width,height))\n",
    "        #se la trasfromazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        #convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "        #restituiamo un dizionario contenente immagine etichetta\n",
    "        return {'image' : im, 'label':label}\n",
    "    #restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dato un oggetto di tipo ScenesDataset restituisce la media e la deviazione standard calcolata su tutte le immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_devst(dataset):\n",
    "    m = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        m+= np.array(sample['image'].sum(1).sum(1)) #accumuliamo la somma dei pixel canale per canale\n",
    "    #dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "    m=m/(len(dataset)*width*height)\n",
    "    #procedura simile per calcolare la deviazione standard\n",
    "    s = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        s+= np.array(((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1))\n",
    "    s=np.sqrt(s/(len(dataset)*width*height))\n",
    "    #print(\"Medie\",m)\n",
    "    #print(\"Dev.Std.\",s)\n",
    "    return m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsImg(name):\n",
    "    if str.__contains__(name, \".jpg\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download immagini da flickr tramite Api search.\n",
    "- Inserire FLICKR_PUBLIC e FLICKR_SECRET personali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FLICKR_PUBLIC = 'fdb1863f5162502cf08eafc41ddc43e5'\n",
    "FLICKR_SECRET = '1bcbf94ddbf054d9'\n",
    "\n",
    "flickr = FlickrAPI(FLICKR_PUBLIC, FLICKR_SECRET, format='parsed-json')\n",
    "extras='url_m'\n",
    "\n",
    "for pg in range(1,11):\n",
    "    cats = flickr.photos.search(text='cat', page=pg, per_page=500, extras=extras, safe_search=1, content_type = 1, sort=\"interestingness-desc\")\n",
    "    photos_cats = cats['photos']\n",
    "    if pg == 1:\n",
    "        count = 0\n",
    "    else:\n",
    "        count = pg*500-501\n",
    "    for i in cats['photos']['photo']:\n",
    "        try:\n",
    "            count += 1\n",
    "            url = i['url_m']\n",
    "            image = io.imread(url)\n",
    "            #io.imshow(image)\n",
    "            #io.show()\n",
    "            io.imsave(\"cat/cat_\" + str(count) + \".jpg\", np.array(image))\n",
    "            print(\"Cat number-->\", count, \"saved.\")\n",
    "        except:\n",
    "            print(\"*******************\")\n",
    "            print(\"Errore...\")\n",
    "            print(\"*******************\")\n",
    "    \n",
    "    dogs = flickr.photos.search(text='dog', page=pg, per_page=500, extras=extras, safe_search=1, content_type = 1, sort=\"interestingness-desc\")\n",
    "    photos_dogs = dogs['photos']\n",
    "    if pg == 1:\n",
    "        count = 0\n",
    "    else:\n",
    "        count = pg*500-501\n",
    "    for i in dogs['photos']['photo']:\n",
    "        try:\n",
    "            count += 1\n",
    "            url = i['url_m']\n",
    "            image = io.imread(url)\n",
    "            #io.imshow(image)\n",
    "            #io.show()\n",
    "            io.imsave(\"dog/dog_\" + str(count) + \".jpg\", np.array(image))\n",
    "            print(\"Dog number-->\", count, \"saved.\")\n",
    "        except:\n",
    "            print(\"*******************\")\n",
    "            print(\"Errore...\")\n",
    "            print(\"*******************\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo i file train e test txt con le relative classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"train.txt\",\"w+\")\n",
    "test = open(\"test.txt\",\"w+\")\n",
    "random.seed(1234)\n",
    "entries = os.listdir('cat/')\n",
    "stop = 0\n",
    "for entry in entries:\n",
    "    #if( stop == 200):\n",
    "        #break\n",
    "    stop += 1\n",
    "    if IsImg(entry):\n",
    "        if(random.randint(1, 100) <= 70):\n",
    "            train.write(\"cat/\" + entry + \", \" + str(0) +\"\\n\")\n",
    "        else:\n",
    "            test.write(\"cat/\" + entry + \", \" + str(0) + \"\\n\")\n",
    "\n",
    "entries = os.listdir('dog/')\n",
    "stop = 0\n",
    "for entry in entries:\n",
    "    #if( stop == 200):\n",
    "        #break\n",
    "    stop += 1\n",
    "    if IsImg(entry):        \n",
    "        if(random.randint(1, 100) <= 70):\n",
    "            train.write(\"dog/\" + entry + \", \" + str(1) +\"\\n\")\n",
    "        else:\n",
    "            test.write(\"dog/\" + entry + \", \" + str(1) +\"\\n\")\n",
    "\n",
    "train.close()\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo l'oggetto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsample = dataset_train[0]\\n#l'immagine è 3 x 256 x 256 perché è una immagine a colori\\nprint(sample)\\nprint(sample['image'].shape)\\nprint(sample['label'])\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decommentare per ricalcolare se cambiano le immagini.\n",
    "dataset_train = ScenesDataset('','train.txt',transform=transforms.ToTensor())\n",
    "#mean_pre_trained, std_pre_trained = get_mean_devst(dataset_train)\n",
    "\n",
    "#Database normalizzato per squeezenet preallenata\n",
    "#print(mean_pre_trained)\n",
    "#print(std_pre_trained)\n",
    "mean_pre_trained = [0.47607468, 0.44641235, 0.39604222]\n",
    "std_pre_trained = [0.27902635, 0.27250564, 0.27781429]\n",
    "\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_squeezenet, std_squeezenet),\n",
    "                                  ])\n",
    "dataset_train_squeezenet = ScenesDataset('','train.txt',transform=transformss)\n",
    "dataset_test_squeezenet = ScenesDataset('','test.txt',transform=transformss)\n",
    "\n",
    "#Database normalizzato per regressione logistica.\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_pre_trained, std_pre_trained),\n",
    "                                  ])\n",
    "dataset_train_logistic_regression = ScenesDataset('','train.txt',transform=transformss)\n",
    "dataset_test_logistic_regression = ScenesDataset('','test.txt',transform=transformss)\n",
    "\n",
    "\n",
    "'''\n",
    "sample = dataset_train[0]\n",
    "#l'immagine è 3 x 256 x 256 perché è una immagine a colori\n",
    "print(sample)\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo dataset per calcolo accuracy su immagini mai viste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pre_trained = [0.47607468, 0.44641235, 0.39604222]\n",
    "std_pre_trained = [0.27902635, 0.27250564, 0.27781429]\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_squeezenet, std_squeezenet),\n",
    "                                  ])\n",
    "dataset_mai_viste_squeezenet = ScenesDataset('','train_immagini_mai_viste.txt',transform=transformss)\n",
    "\n",
    "#Database normalizzato per regressione logistica.\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_pre_trained, std_pre_trained),\n",
    "                                  ])\n",
    "dataset_mai_viste_logistic_regression = ScenesDataset('','train_immagini_mai_viste.txt',transform=transformss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo l'oggetto DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_squeezenet = DataLoader(dataset_train_squeezenet, batch_size=20, num_workers=0, shuffle=True)\n",
    "test_loader_squeezenet = DataLoader(dataset_test_squeezenet, batch_size=20, num_workers=0)\n",
    "train_loader_mai_viste_squeezenet = DataLoader(dataset_mai_viste_squeezenet, batch_size=20, num_workers=0)\n",
    "\n",
    "train_loader_logistic_regression = DataLoader(dataset_train_logistic_regression, batch_size=20, num_workers=0, shuffle=True)\n",
    "test_loader_logistic_regression = DataLoader(dataset_test_logistic_regression, batch_size=20, num_workers=0)\n",
    "train_loader_mai_viste_logistic_regression = DataLoader(dataset_mai_viste_logistic_regression, batch_size=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, in_features, ou_features):\n",
    "        \"\"\"Costruisce un regressore logistico.\n",
    "        Input:\n",
    "        in_features: numero di feature in input (es. 30)\"\"\"\n",
    "        super(LogisticRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_features, ou_features) #il regressore logistico restituisce probabilità\n",
    "        #quindi il numero di feature di output è \"1\"\n",
    "        self.logistic = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        logits = self.linear(x)\n",
    "        if self.training: #se il modulo è in fase di training\n",
    "        #la proprietà \"training\" è messa a disposizione dai meccanismi\n",
    "        #automatici dei moduli di PyTorch\n",
    "            return logits\n",
    "        else: #se siamo in fase di test, calcoliamo le probabilità\n",
    "            return self.logistic(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variabili di controllo per l'allenamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(dataset_train) / batch_size)\n",
    "input_dim = width*height*3\n",
    "output_dim = 2\n",
    "lr_rate = 0.0001\n",
    "channel = 3\n",
    "momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_regression = LogisticRegression(input_dim, output_dim)\n",
    "model_squeezenet = models.squeezenet1_0(pretrained=True)#(input_dim, output_dim)\n",
    "model_alexnet = models.alexnet(pretrained=True)#(input_dim, output_dim)\n",
    "model_alexnet.classifier[6] = nn.Linear(4096, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(model, train_loader, test_loader, lr=0.001, epochs=20, momentum=0.8, exp_name = 'experiment', logistic_regression = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(),lr, momentum=momentum)\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "    \n",
    "    \n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    loss_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Loss', 'legend':['train','test']})\n",
    "    acc_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Accuracy','legend':['train','test']})\n",
    "    visdom_saver = VisdomSaver(envs=[exp_name])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        #print(\"Primo ciclo for.\")\n",
    "        for mode in ['train', 'test']:\n",
    "            #print(\"Secondo ciclo for.\")\n",
    "            \n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "            \n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "            #print(\"Mode-->\",mode)\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                #trasformiamo i tensori in variabili\n",
    "                if logistic_regression == True:\n",
    "                    x=Variable(batch['image'].view(-1, width*height*channel), requires_grad=(mode=='train'))\n",
    "                else:\n",
    "                    x=Variable(batch['image'], requires_grad=(mode=='train'))\n",
    "\n",
    "                y=Variable(batch['label'])\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                    #print(\"Con cuda\")\n",
    "                #else:\n",
    "                    #print(\"Senza cuda\")\n",
    "                output = model(x)\n",
    "                #print(output)\n",
    "                #print(y)\n",
    "                l = criterion(output,y)\n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                #print(\"L-->\",l.item())\n",
    "                acc = accuracy_score(y.cpu().data,output.cpu().max(1)[1].data)\n",
    "                epoch_loss+=l.data.item()*x.shape[0]\n",
    "                epoch_acc+=acc*x.shape[0]\n",
    "                samples+=x.shape[0]\n",
    "                '''print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "                (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples),\n",
    "                epoch_loss/samples,\n",
    "                epoch_acc/samples,\n",
    "                losses[mode].append(epoch_loss))'''\n",
    "                accuracies[mode].append(epoch_acc)\n",
    "                n = batch['image'].shape[0]\n",
    "                loss_meter.add(l.item()*n,n)\n",
    "                acc_meter.add(acc*n,n)\n",
    "                #loss_logger.log(e+(i+1)/len(loaders[mode]), loss_meter.value()[0], name=mode)\n",
    "                #acc_logger.log(e+(i+1)/len(loaders[mode]), acc_meter.value()[0], name=mode)\n",
    "\n",
    "\n",
    "            loss_logger.log(e+1, loss_meter.value()[0], name=mode)\n",
    "            acc_logger.log(e+1, acc_meter.value()[0], name=mode)\n",
    "            if mode == \"train\":\n",
    "                print(e, end = \" \")\n",
    "            #print(\"Fine secondo ciclo for\")\n",
    "        '''print(\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "        (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))'''\n",
    "        torch.save(model.state_dict(), \"./\" + exp_name + \".pth\")\n",
    "        print(\"Pesi aggiornati.\")\n",
    "    #print(\"Ho finito.\")\n",
    "    #restituiamo il modello e i vari log\n",
    "    return model, (losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(model_logistic_regression, \n",
    "                                                     epochs = 50, \n",
    "                                                     train_loader = train_loader_logistic_regression,\n",
    "                                                     test_loader = test_loader_logistic_regression, \n",
    "                                                     logistic_regression = True,\n",
    "                                                     exp_name = \"Train_logisticRegression_lr\" + str(lr_rate) + \"_momentum\" + str(momentum) + \"_epochs\" + str(int(epochs)), \n",
    "                                                     lr = lr_rate, \n",
    "                                                     momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_squeezenet, result_logs_squeezenet = train_classification(model_squeezenet, \n",
    "                                                     epochs = 50, \n",
    "                                                     train_loader = train_loader_squeezenet,\n",
    "                                                     test_loader = test_loader_squeezenet, \n",
    "                                                     logistic_regression = False,\n",
    "                                                     exp_name = \"Train_squeezenet_lr\" + str(lr_rate) + \"_momentum\" + str(momentum) + \"_epochs\" + str(int(epochs)), \n",
    "                                                     lr = lr_rate, \n",
    "                                                     momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n",
      "WARNING:root:Setting up a new session...\n",
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Pesi aggiornati.\n",
      "1 Pesi aggiornati.\n",
      "2 Pesi aggiornati.\n",
      "3 Pesi aggiornati.\n",
      "4 Pesi aggiornati.\n",
      "5 Pesi aggiornati.\n",
      "6 Pesi aggiornati.\n",
      "7 Pesi aggiornati.\n",
      "8 Pesi aggiornati.\n",
      "9 Pesi aggiornati.\n",
      "10 Pesi aggiornati.\n",
      "11 Pesi aggiornati.\n",
      "12 Pesi aggiornati.\n",
      "13 Pesi aggiornati.\n",
      "14 Pesi aggiornati.\n",
      "15 Pesi aggiornati.\n",
      "16 Pesi aggiornati.\n",
      "17 Pesi aggiornati.\n",
      "18 Pesi aggiornati.\n",
      "19 Pesi aggiornati.\n",
      "20 Pesi aggiornati.\n",
      "21 Pesi aggiornati.\n",
      "22 Pesi aggiornati.\n",
      "23 Pesi aggiornati.\n",
      "24 Pesi aggiornati.\n",
      "25 Pesi aggiornati.\n"
     ]
    }
   ],
   "source": [
    "result_alexnet, result_logs_alexnet = train_classification(model_alexnet, \n",
    "                                                     epochs = 50, \n",
    "                                                     train_loader = train_loader_squeezenet,\n",
    "                                                     test_loader = test_loader_squeezenet, \n",
    "                                                     logistic_regression = False,\n",
    "                                                     exp_name = \"NuovaProva_Train_alexnet_lr\" + str(lr_rate) + \"_momentum\" + str(momentum) + \"_epochs\" + str(int(epochs)), \n",
    "                                                     lr = lr_rate, \n",
    "                                                     momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(net, path):\n",
    "    my_file = Path(path) #Carica il file(lo state potenzialmente)\n",
    "    if my_file.is_file(): #Carico lo state altrimenti rialleno\n",
    "        net.load_state_dict(torch.load(path))\n",
    "        print(\"Caricato \" + path)\n",
    "    else: \n",
    "        print(\"File mancante.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricato ./Train_alexnet_lr0.0001_momentum0.8_epochs53.pth\n",
      "Caricato ./Train_squeezenet_lr0.0001_momentum0.8_epochs53.pth\n",
      "Caricato ./Train_logisticRegression_lr0.0001_momentum0.8_epochs53.pth\n"
     ]
    }
   ],
   "source": [
    "load_state(model_alexnet, \"./Train_alexnet_lr0.0001_momentum0.8_epochs53.pth\")\n",
    "load_state(model_squeezenet, \"./Train_squeezenet_lr0.0001_momentum0.8_epochs53.pth\")\n",
    "load_state(model_logistic_regression, \"./Train_logisticRegression_lr0.0001_momentum0.8_epochs53.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_classification(model, test_loader, logistic_regression = True):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch in test_loader:\n",
    "        #x=Variable(batch[\"image\"])\n",
    "        if logistic_regression == True:\n",
    "            x=Variable(batch['image'].view(-1, width*height*channel))\n",
    "        else:\n",
    "            x=Variable(batch['image'])\n",
    "        x = x.cpu()\n",
    "        #applichiamo la funzione softmax per avere delle probabilità\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            model.cuda()\n",
    "        pred = softmax(model(x)).data.cpu().numpy().copy()\n",
    "        gt = batch[\"label\"].cpu().numpy().copy()\n",
    "        #print(\"Pred-->\", pred, \", gt-->\", gt)\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "        #print(len(preds), len(gts))\n",
    "    return np.concatenate(preds),np.concatenate(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators(net, loader, train, logistic_regression):\n",
    "    #In questo caso e' l'accuracy per la classe AlexNetRetrain\n",
    "    predictions, groundruth_t = test_model_classification(net, loader, logistic_regression)\n",
    "    if train == True:\n",
    "        print(\"****Indicatori per dataset di Train****\")\n",
    "    else:\n",
    "        print(\"****Indicatori per dataset di Test****\")\n",
    "    print (\"Accuracy: %0.2f\" % \\\n",
    "    accuracy_score(groundruth_t,predictions.argmax(1)))\n",
    "    print(\"Precision_score-->\")\n",
    "    print(precision_score(groundruth_t,predictions.argmax(1), average = 'macro'))\n",
    "    print(\"Recall_score-->\")\n",
    "    print(recall_score(groundruth_t,predictions.argmax(1), average = 'macro'))\n",
    "    print(\"F1_Score-->\")\n",
    "    print(f1_score(groundruth_t,predictions.argmax(1), average = 'macro'))\n",
    "    print(\"Matrice di confusione-->\\n\")\n",
    "    print(confusion_matrix(groundruth_t,predictions.argmax(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ Logistic Regression ++++++++++\n",
      "****Indicatori per dataset di Train****\n",
      "Accuracy: 0.86\n",
      "Precision_score-->\n",
      "0.855573127937673\n",
      "Recall_score-->\n",
      "0.8555645427400986\n",
      "F1_Score-->\n",
      "0.8555653923541247\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[856 142]\n",
      " [146 850]]\n",
      "++++++++++ Squeezenet ++++++++++\n",
      "****Indicatori per dataset di Train****\n",
      "Accuracy: 0.99\n",
      "Precision_score-->\n",
      "0.991985786491533\n",
      "Recall_score-->\n",
      "0.9919739076546668\n",
      "F1_Score-->\n",
      "0.9919758551307847\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[992   6]\n",
      " [ 10 986]]\n",
      "++++++++++ AlexNet ++++++++++\n",
      "****Indicatori per dataset di Train****\n",
      "Accuracy: 1.00\n",
      "Precision_score-->\n",
      "0.998\n",
      "Recall_score-->\n",
      "0.9979959919839679\n",
      "F1_Score-->\n",
      "0.9979939799277269\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[994   4]\n",
      " [  0 996]]\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------ Accuracy su immagini mai viste prima ------------------------------------------\")\n",
    "print(\"++++++++++ Logistic Regression ++++++++++\")\n",
    "get_indicators(model_logistic_regression, train_loader_mai_viste_logistic_regression, True, True)\n",
    "print(\"++++++++++ Squeezenet ++++++++++\")\n",
    "get_indicators(model_squeezenet, train_loader_mai_viste_squeezenet, True, False)\n",
    "print(\"++++++++++ AlexNet ++++++++++\")\n",
    "get_indicators(model_alexnet, train_loader_mai_viste_squeezenet, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------ Accuracy su immagini di train ------------------------------------------\n",
      "++++++++++ Logistic Regression ++++++++++\n",
      "****Indicatori per dataset di Train****\n",
      "Accuracy: 1.00\n",
      "Precision_score-->\n",
      "0.9994596184813576\n",
      "Recall_score-->\n",
      "0.9994627763424431\n",
      "F1_Score-->\n",
      "0.9994611651664571\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[2758    1]\n",
      " [   2 2807]]\n",
      "++++++++++ Squeezenet ++++++++++\n",
      "****Indicatori per dataset di Train****\n",
      "Accuracy: 1.00\n",
      "Precision_score-->\n",
      "1.0\n",
      "Recall_score-->\n",
      "1.0\n",
      "F1_Score-->\n",
      "1.0\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[2759    0]\n",
      " [   0 2809]]\n",
      "++++++++++ AlexNet ++++++++++\n",
      "****Indicatori per dataset di Train****\n",
      "Accuracy: 1.00\n",
      "Precision_score-->\n",
      "1.0\n",
      "Recall_score-->\n",
      "1.0\n",
      "F1_Score-->\n",
      "1.0\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[2759    0]\n",
      " [   0 2809]]\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------ Accuracy su immagini di train ------------------------------------------\")\n",
    "print(\"++++++++++ Logistic Regression ++++++++++\")\n",
    "get_indicators(model_logistic_regression, train_loader_logistic_regression, True, True)\n",
    "print(\"++++++++++ Squeezenet ++++++++++\")\n",
    "get_indicators(model_squeezenet, train_loader_squeezenet, True, False)\n",
    "print(\"++++++++++ AlexNet ++++++++++\")\n",
    "get_indicators(model_alexnet, train_loader_squeezenet, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------ Accuracy su immagini di test ------------------------------------------\n",
      "++++++++++ Logistic Regression ++++++++++\n",
      "****Indicatori per dataset di Test****\n",
      "Accuracy: 0.57\n",
      "Precision_score-->\n",
      "0.5726576994434137\n",
      "Recall_score-->\n",
      "0.5726591437677664\n",
      "F1_Score-->\n",
      "0.5726489355358101\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[705 530]\n",
      " [520 702]]\n",
      "++++++++++ Squeezenet ++++++++++\n",
      "****Indicatori per dataset di Test****\n",
      "Accuracy: 0.98\n",
      "Precision_score-->\n",
      "0.9816827924001215\n",
      "Recall_score-->\n",
      "0.9816866224481007\n",
      "F1_Score-->\n",
      "0.9816845447972031\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[1212   23]\n",
      " [  22 1200]]\n",
      "++++++++++ AlexNet ++++++++++\n",
      "****Indicatori per dataset di Test****\n",
      "Accuracy: 0.98\n",
      "Precision_score-->\n",
      "0.9816827924001215\n",
      "Recall_score-->\n",
      "0.9816866224481007\n",
      "F1_Score-->\n",
      "0.9816845447972031\n",
      "Matrice di confusione-->\n",
      "\n",
      "[[1212   23]\n",
      " [  22 1200]]\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------ Accuracy su immagini di test ------------------------------------------\")\n",
    "print(\"++++++++++ Logistic Regression ++++++++++\")\n",
    "get_indicators(model_logistic_regression, test_loader_logistic_regression, False, True)\n",
    "print(\"++++++++++ Squeezenet ++++++++++\")\n",
    "get_indicators(model_squeezenet, test_loader_squeezenet, False, False)\n",
    "print(\"++++++++++ AlexNet ++++++++++\")\n",
    "get_indicators(model_alexnet, test_loader_squeezenet, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
