{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessari:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flickrapi import FlickrAPI\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "import urllib\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torchnet.meter import AverageValueMeter\n",
    "from torchnet.logger import VisdomPlotLogger, VisdomSaver\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dichiarazione costanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "#Media e deviazione standard precalcolata per Squeezenet.\n",
    "mean_squeezenet = [0.485, 0.456, 0.406]\n",
    "std_squeezenet = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dichiarazione di classi e funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenesDataset(Dataset):\n",
    "    def __init__(self,base_path,txt_list,transform=None):\n",
    "        #conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path=base_path\n",
    "        #carichiamo la lista dei file\n",
    "        #sarà una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        #conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        #recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "        #carichiamo l'immagine utilizzando PIL\n",
    "        im = Image.open(path.join(self.base_path, f)).convert('RGB')\n",
    "        #Resize:\n",
    "        im = im.resize((width,height))\n",
    "        #se la trasfromazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        #convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "        #restituiamo un dizionario contenente immagine etichetta\n",
    "        return {'image' : im, 'label':label}\n",
    "    #restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dato un oggetto di tipo ScenesDataset restituisce la media e la deviazione standard calcolata su tutte le immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_devst(dataset):\n",
    "    m = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        m+= np.array(sample['image'].sum(1).sum(1)) #accumuliamo la somma dei pixel canale per canale\n",
    "    #dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "    m=m/(len(dataset)*width*height)\n",
    "    #procedura simile per calcolare la deviazione standard\n",
    "    s = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        s+= np.array(((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1))\n",
    "    s=np.sqrt(s/(len(dataset)*width*height))\n",
    "    #print(\"Medie\",m)\n",
    "    #print(\"Dev.Std.\",s)\n",
    "    return m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsImg(name):\n",
    "    if str.__contains__(name, \".jpg\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download immagini da flickr tramite Api search.\n",
    "- Inserire FLICKR_PUBLIC e FLICKR_SECRET personali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FLICKR_PUBLIC = 'fdb1863f5162502cf08eafc41ddc43e5'\n",
    "FLICKR_SECRET = '1bcbf94ddbf054d9'\n",
    "\n",
    "flickr = FlickrAPI(FLICKR_PUBLIC, FLICKR_SECRET, format='parsed-json')\n",
    "extras='url_m'\n",
    "\n",
    "for pg in range(1,11):\n",
    "    cats = flickr.photos.search(text='cat', page=pg, per_page=500, extras=extras, safe_search=1, content_type = 1, sort=\"interestingness-desc\")\n",
    "    photos_cats = cats['photos']\n",
    "    if pg == 1:\n",
    "        count = 0\n",
    "    else:\n",
    "        count = pg*500-501\n",
    "    for i in cats['photos']['photo']:\n",
    "        try:\n",
    "            count += 1\n",
    "            url = i['url_m']\n",
    "            image = io.imread(url)\n",
    "            #io.imshow(image)\n",
    "            #io.show()\n",
    "            io.imsave(\"cat/cat_\" + str(count) + \".jpg\", np.array(image))\n",
    "            print(\"Cat number-->\", count, \"saved.\")\n",
    "        except:\n",
    "            print(\"*******************\")\n",
    "            print(\"Errore...\")\n",
    "            print(\"*******************\")\n",
    "    \n",
    "    dogs = flickr.photos.search(text='dog', page=pg, per_page=500, extras=extras, safe_search=1, content_type = 1, sort=\"interestingness-desc\")\n",
    "    photos_dogs = dogs['photos']\n",
    "    if pg == 1:\n",
    "        count = 0\n",
    "    else:\n",
    "        count = pg*500-501\n",
    "    for i in dogs['photos']['photo']:\n",
    "        try:\n",
    "            count += 1\n",
    "            url = i['url_m']\n",
    "            image = io.imread(url)\n",
    "            #io.imshow(image)\n",
    "            #io.show()\n",
    "            io.imsave(\"dog/dog_\" + str(count) + \".jpg\", np.array(image))\n",
    "            print(\"Dog number-->\", count, \"saved.\")\n",
    "        except:\n",
    "            print(\"*******************\")\n",
    "            print(\"Errore...\")\n",
    "            print(\"*******************\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo i file train e test txt con le relative classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"train.txt\",\"w+\")\n",
    "test = open(\"test.txt\",\"w+\")\n",
    "\n",
    "entries = os.listdir('cat/')\n",
    "for entry in entries:\n",
    "    if IsImg(entry):\n",
    "        if(random.randint(1, 100) <= 70):\n",
    "            train.write(\"cat/\" + entry + \", \" + str(0) +\"\\n\")\n",
    "        else:\n",
    "            test.write(\"cat/\" + entry + \", \" + str(0) + \"\\n\")\n",
    "\n",
    "entries = os.listdir('dog/')\n",
    "for entry in entries:\n",
    "    if IsImg(entry):        \n",
    "        if(random.randint(1, 100) <= 70):\n",
    "            train.write(\"dog/\" + entry + \", \" + str(1) +\"\\n\")\n",
    "        else:\n",
    "            test.write(\"dog/\" + entry + \", \" + str(1) +\"\\n\")\n",
    "\n",
    "train.close()\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo l'oggetto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsample = dataset_train[0]\\n#l'immagine è 3 x 256 x 256 perché è una immagine a colori\\nprint(sample)\\nprint(sample['image'].shape)\\nprint(sample['label'])\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decommentare per ricalcolare se cambiano le immagini.\n",
    "#dataset_train = ScenesDataset('','train.txt',transform=transforms.ToTensor())\n",
    "#mean_pre_trained, std_pre_trained = get_mean_devst(dataset_train)\n",
    "\n",
    "#Database normalizzato per squeezenet preallenata\n",
    "#print(mean_pre_trained)\n",
    "#print(std_pre_trained)\n",
    "mean_pre_trained = [0.47607468, 0.44641235, 0.39604222]\n",
    "std_pre_trained = [0.27902635, 0.27250564, 0.27781429]\n",
    "\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_squeezenet, std_squeezenet),\n",
    "                                  ])\n",
    "dataset_train_squeezenet = ScenesDataset('','train.txt',transform=transformss)\n",
    "dataset_test_squeezenet = ScenesDataset('','test.txt',transform=transformss)\n",
    "\n",
    "#Database normalizzato per regressione logistica.\n",
    "transformss = transforms.Compose([\n",
    "                                      #transforms.Resize((width,height)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize(mean_pre_trained,std_pre_trained),\n",
    "                                      transforms.Normalize(mean_pre_trained, std_pre_trained),\n",
    "                                  ])\n",
    "dataset_train_logistic_regression = ScenesDataset('','train.txt',transform=transformss)\n",
    "dataset_test_logistic_regression = ScenesDataset('','test.txt',transform=transformss)\n",
    "\n",
    "\n",
    "'''\n",
    "sample = dataset_train[0]\n",
    "#l'immagine è 3 x 256 x 256 perché è una immagine a colori\n",
    "print(sample)\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creo l'oggetto DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_squeezenet = DataLoader(dataset_train_squeezenet, batch_size=20, num_workers=0, shuffle=True)\n",
    "test_loader_squeezenet = DataLoader(dataset_test_squeezenet, batch_size=20, num_workers=0)\n",
    "\n",
    "train_loader_logistic_regression = DataLoader(dataset_train_logistic_regression, batch_size=20, num_workers=0, shuffle=True)\n",
    "test_loader_logistic_regression = DataLoader(dataset_test_logistic_regression, batch_size=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, in_features, ou_features):\n",
    "        \"\"\"Costruisce un regressore logistico.\n",
    "        Input:\n",
    "        in_features: numero di feature in input (es. 30)\"\"\"\n",
    "        super(LogisticRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_features, ou_features) #il regressore logistico restituisce probabilità\n",
    "        #quindi il numero di feature di output è \"1\"\n",
    "        self.logistic = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        logits = self.linear(x)\n",
    "        if self.training: #se il modulo è in fase di training\n",
    "        #la proprietà \"training\" è messa a disposizione dai meccanismi\n",
    "        #automatici dei moduli di PyTorch\n",
    "            return logits\n",
    "        else: #se siamo in fase di test, calcoliamo le probabilità\n",
    "            return self.logistic(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variabili di controllo per l'allenamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(dataset_train) / batch_size)\n",
    "input_dim = width*height*3\n",
    "output_dim = 2\n",
    "lr_rate = 0.001\n",
    "channel = 3\n",
    "momentum = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_regression = LogisticRegression(input_dim, output_dim)\n",
    "model_squeezenet = models.squeezenet1_0(pretrained=True)#(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(model, train_loader, test_loader, lr=0.001, epochs=20, momentum=0.8, exp_name = 'experiment', logistic_regression = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(),lr, momentum=momentum)\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "    \n",
    "    \n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    loss_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Loss', 'legend':['train','test']})\n",
    "    acc_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Accuracy','legend':['train','test']})\n",
    "    visdom_saver = VisdomSaver(envs=[exp_name])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        #print(\"Primo ciclo for.\")\n",
    "        for mode in ['train', 'test']:\n",
    "            #print(\"Secondo ciclo for.\")\n",
    "            \n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "            \n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "            #print(\"Mode-->\",mode)\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                #trasformiamo i tensori in variabili\n",
    "                if logistic_regression == True:\n",
    "                    x=Variable(batch['image'].view(-1, width*height*channel), requires_grad=(mode=='train'))\n",
    "                else:\n",
    "                    x=Variable(batch['image'], requires_grad=(mode=='train'))\n",
    "\n",
    "                y=Variable(batch['label'])\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                    #print(\"Con cuda\")\n",
    "                #else:\n",
    "                    #print(\"Senza cuda\")\n",
    "                output = model(x)\n",
    "                #print(output)\n",
    "                #print(y)\n",
    "                l = criterion(output,y)\n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                #print(\"L-->\",l.item())\n",
    "                acc = accuracy_score(y.cpu().data,output.cpu().max(1)[1].data)\n",
    "                epoch_loss+=l.data.item()*x.shape[0]\n",
    "                epoch_acc+=acc*x.shape[0]\n",
    "                samples+=x.shape[0]\n",
    "                '''print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "                (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples),\n",
    "                epoch_loss/samples,\n",
    "                epoch_acc/samples,\n",
    "                losses[mode].append(epoch_loss))'''\n",
    "                accuracies[mode].append(epoch_acc)\n",
    "                n = batch['image'].shape[0]\n",
    "                loss_meter.add(l.item()*n,n)\n",
    "                acc_meter.add(acc*n,n)\n",
    "                #loss_logger.log(e+(i+1)/len(loaders[mode]), loss_meter.value()[0], name=mode)\n",
    "                #acc_logger.log(e+(i+1)/len(loaders[mode]), acc_meter.value()[0], name=mode)\n",
    "\n",
    "\n",
    "            loss_logger.log(e+1, loss_meter.value()[0], name=mode)\n",
    "            acc_logger.log(e+1, acc_meter.value()[0], name=mode)\n",
    "            if mode == \"train\":\n",
    "                print(e, end = \" \")\n",
    "            #print(\"Fine secondo ciclo for\")\n",
    "        '''print(\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "        (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))'''\n",
    "        torch.save(model.state_dict(), \"./\" + exp_name + \".pth\")\n",
    "        print(\"Pesi aggiornati.\")\n",
    "    #print(\"Ho finito.\")\n",
    "    #restituiamo il modello e i vari log\n",
    "    return model, (losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n",
      "WARNING:root:Setting up a new session...\n",
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Pesi aggiornati.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9217d78fd4cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m lenet_mnist, lenet_mnist_logs = train_classification(model_logistic_regression, epochs = 20, train_loader = train_loader_squeezenet,\n\u001b[0;32m      2\u001b[0m                                                                 \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_loader_squeezenet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic_regression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                                 exp_name = \"Train_logistic_regression\", lr = lr_rate, momentum = momentum)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-019a14c63160>\u001b[0m in \u001b[0;36mtrain_classification\u001b[1;34m(model, train_loader, test_loader, lr, epochs, momentum, exp_name, logistic_regression)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m#print(\"Mode-->\",mode)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[1;31m#trasformiamo i tensori in variabili\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlogistic_regression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ba2ccb3541a1>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#se la trasfromazione è definita, applichiamola all'immagine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m#convertiamo l'etichetta in un intero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(model_logistic_regression, epochs = 20, train_loader = train_loader_logistic_regression,\n",
    "                                                                test_loader = test_loader_logistic_regression, logistic_regression = True,\n",
    "                                                                exp_name = \"Train_logistic_regression\", lr = lr_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "nn5 = KNN(5)\n",
    "nn5.fit(X2_train,y2_train)\n",
    "predicted_labels=nn5.predict(X2_test)\n",
    "a = accuracy_score(y2_test,predicted_labels)\n",
    "M = confusion_matrix(y2_test,predicted_labels)\n",
    "print(\"5-NN, accuracy: %0.2f, Confusion Matrix:\\n\" %a)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    # resize the image to a fixed size, then flatten the image into\n",
    "    # a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
    "    [0, 180, 0, 256, 0, 256])\n",
    "\n",
    "    # handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "\n",
    "    # otherwise, perform \"in place\" normalization in OpenCV 3 (I\n",
    "    # personally hate the way this is done\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "\n",
    "    # return the flattened histogram as the feature vector\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
